import requests
import pandas as pd
import io
import json
from datetime import datetime, timedelta
from urllib.parse import quote

# === CONFIG ===
STATIONS_CSV = "stations.csv"
OUTPUT_GEOJSON = "air_data_gouv.geojson"
WANTED_POLLUTANTS = []  # vide = tous

def build_e2_url(date: datetime):
    """
    Construire l'URL de t√©l√©chargement via l'API MinIO
    """
    date_str = date.strftime("%Y-%m-%d")
    year_str = date.strftime("%Y")
    
    # Chemin du fichier (sans encoding ici, on l'encode apr√®s)
    file_path = f"lcsqa/concentrations-de-polluants-atmospheriques-reglementes/temps-reel/{year_str}/FR_E2_{date_str}.csv"
    
    # URL encod√©e
    file_path_encoded = quote(file_path, safe='')
    
    return f"https://object.infra.data.gouv.fr/api/v1/buckets/ineris-prod/objects/download?prefix={file_path_encoded}"

def download_csv(url):
    """
    T√©l√©charge le fichier et retourne un DataFrame
    """
    print(f"T√©l√©chargement : {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/csv,application/csv,text/plain,*/*',
    }
    
    try:
        r = requests.get(url, headers=headers, timeout=30)
        
        print(f"Status: {r.status_code}")
        print(f"Content-Type: {r.headers.get('Content-Type')}")
        print(f"Taille: {len(r.content)} octets")
        
        if r.status_code == 200 and len(r.content) > 100:
            # Afficher aper√ßu
            print("=== APER√áU (200 premiers caract√®res) ===")
            print(r.text[:200])
            print("=========================================")
            
            # Parser le CSV
            try:
                df = pd.read_csv(io.StringIO(r.text), sep=";")
                
                if df.empty:
                    print("‚ö†Ô∏è Vide avec sep=';', test avec ','")
                    df = pd.read_csv(io.StringIO(r.text), sep=",")
                
                print(f"‚úÖ CSV pars√© : {len(df)} lignes, {len(df.columns)} colonnes")
                if not df.empty:
                    print("Premi√®res colonnes :", df.columns.tolist()[:5])
                return df
            except Exception as e:
                print(f"‚ùå Erreur parsing CSV : {e}")
                return pd.DataFrame()
        else:
            print(f"‚ùå Fichier vide ou erreur HTTP")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"‚ùå Erreur requ√™te : {e}")
        return pd.DataFrame()

# ============ LOGIQUE PRINCIPALE ============

# Chercher le fichier le plus r√©cent
target_date = datetime.utcnow().date() - timedelta(days=1)
print(f"\nüîç Recherche du fichier pour le {target_date}")

df_measures = download_csv(build_e2_url(datetime.combine(target_date, datetime.min.time())))

# Retry sur les jours pr√©c√©dents si n√©cessaire
tries = 5
i = 1
while df_measures.empty and i < tries:
    test_date = target_date - timedelta(days=i)
    print(f"\nüîç Tentative avec {test_date}")
    df_measures = download_csv(build_e2_url(datetime.combine(test_date, datetime.min.time())))
    i += 1

if df_measures.empty:
    print("\n‚ùå Aucun fichier E2 valide trouv√© sur les 5 derniers jours.")
    exit(1)

print("\n‚úÖ Fichier de mesures charg√© !")
print(f"Colonnes disponibles : {df_measures.columns.tolist()}")

# Filtrer polluants si sp√©cifi√©
if WANTED_POLLUTANTS:
    df_measures = df_measures[df_measures["Polluant"].isin(WANTED_POLLUTANTS)]
    print(f"Filtrage polluants : {len(df_measures)} lignes restantes")

if df_measures.empty:
    print("‚ùå Aucune donn√©e apr√®s filtrage des polluants")
    exit(1)

# Lire stations locales
print(f"\nüìç Chargement du fichier stations : {STATIONS_CSV}")
try:
    df_stations = pd.read_csv(STATIONS_CSV, sep=";")
    print(f"Stations charg√©es : {len(df_stations)} lignes")
    print(f"Colonnes stations : {df_stations.columns.tolist()[:5]}")
except Exception as e:
    print(f"‚ùå Erreur lecture stations.csv : {e}")
    exit(1)

# Merge mesures + coords
print("\nüîó Merge des donn√©es...")
df_merged = df_measures.merge(
    df_stations,
    left_on="code site",
    right_on="Code",
    how="left"
)

if df_merged.empty:
    print("‚ùå Merge a √©chou√© - aucune correspondance")
    exit(1)

# Filtrer les lignes sans coordonn√©es
df_merged = df_merged[df_merged['Latitude'].notna() & df_merged['Longitude'].notna()]
print(f"‚úÖ Merge r√©ussi : {len(df_merged)} lignes avec coordonn√©es")

# Cr√©ation GeoJSON
print("\nüó∫Ô∏è  Cr√©ation du GeoJSON...")
features = []

for _, row in df_merged.iterrows():
    features.append({
        "type": "Feature",
        "geometry": {
            "type": "Point",
            "coordinates": [float(row["Longitude"]), float(row["Latitude"])]
        },
        "properties": {
            "code_station": str(row.get("code site", "")),
            "nom_station": str(row.get("Nom site", "")),
            "polluant": str(row.get("Polluant", "")),
            "date": str(row.get("Date de d√©but", "") or row.get("Date", "")),
            "concentration": float(row.get("valeur", 0)) if pd.notna(row.get("valeur")) else None,
            "unite": str(row.get("Unit√© de mesure", "")),
        }
    })

geojson = {"type": "FeatureCollection", "features": features}

with open(OUTPUT_GEOJSON, "w", encoding="utf-8") as f:
    json.dump(geojson, f, ensure_ascii=False, indent=2)

print(f"‚úÖ GeoJSON g√©n√©r√© : {OUTPUT_GEOJSON} ({len(features)} points)")














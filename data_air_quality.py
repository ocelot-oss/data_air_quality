import requests
import pandas as pd
import io
import json
from datetime import datetime, timedelta
from urllib.parse import quote

# === CONFIG ===
STATIONS_CSV = "stations.csv"
OUTPUT_GEOJSON = "air_data_gouv.geojson"
WANTED_POLLUTANTS = []  # vide = tous, ou sp√©cifie : ['NO2', 'PM10', 'O3']

def build_e2_url(date: datetime):
    date_str = date.strftime("%Y-%m-%d")
    year_str = date.strftime("%Y")
    file_path = f"lcsqa/concentrations-de-polluants-atmospheriques-reglementes/temps-reel/{year_str}/FR_E2_{date_str}.csv"
    file_path_encoded = quote(file_path, safe='')
    return f"https://object.infra.data.gouv.fr/api/v1/buckets/ineris-prod/objects/download?prefix={file_path_encoded}"

def download_csv(url):
    print(f"T√©l√©chargement : {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/csv,application/csv,text/plain,*/*',
    }
    
    try:
        r = requests.get(url, headers=headers, timeout=30)
        
        print(f"Status: {r.status_code}")
        print(f"Taille: {len(r.content)} octets")
        
        if r.status_code == 200 and len(r.content) > 100:
            # IMPORTANT : encoding="utf-8-sig" pour nettoyer le BOM
            df = pd.read_csv(io.StringIO(r.text), sep=";", encoding="utf-8-sig")
            
            print(f"‚úÖ CSV pars√© : {len(df)} lignes, {len(df.columns)} colonnes")
            return df
        else:
            print(f"‚ùå Fichier vide ou erreur")
            return pd.DataFrame()
            
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        return pd.DataFrame()

# ============ LOGIQUE PRINCIPALE ============

target_date = datetime.utcnow().date() - timedelta(days=1)
print(f"\nüîç Recherche du fichier pour le {target_date}")

df_measures = download_csv(build_e2_url(datetime.combine(target_date, datetime.min.time())))

# Retry
tries = 5
i = 1
while df_measures.empty and i < tries:
    test_date = target_date - timedelta(days=i)
    print(f"\nüîç Tentative avec {test_date}")
    df_measures = download_csv(build_e2_url(datetime.combine(test_date, datetime.min.time())))
    i += 1

if df_measures.empty:
    print("\n‚ùå Aucun fichier E2 valide trouv√©")
    exit(1)

print("\n‚úÖ Fichier de mesures charg√© !")
print(f"Colonnes : {df_measures.columns.tolist()[:10]}")

# FILTRE VALL√âE DE L'ARVE
print("\nüîç Filtrage pour la vall√©e de l'Arve...")
df_measures = df_measures[
    df_measures['Zas'].str.contains('ARVE', case=False, na=False)
]

print(f"‚úÖ {len(df_measures)} mesures trouv√©es")
print(f"Stations : {df_measures['nom site'].unique()}")
print(f"Polluants : {df_measures['Polluant'].unique()}")

if df_measures.empty:
    print("‚ùå Aucune donn√©e pour la vall√©e de l'Arve")
    exit(1)

# Filtrer polluants si sp√©cifi√©
if WANTED_POLLUTANTS:
    df_measures = df_measures[df_measures["Polluant"].isin(WANTED_POLLUTANTS)]

# Lire stations locales
print(f"\nüìç Chargement du fichier stations : {STATIONS_CSV}")
try:
    df_stations = pd.read_csv(STATIONS_CSV, sep=";")
    print(f"Stations : {len(df_stations)} lignes")
except Exception as e:
    print(f"‚ùå Erreur lecture stations.csv : {e}")
    exit(1)

# Merge
print("\nüîó Merge des donn√©es...")
df_merged = df_measures.merge(
    df_stations,
    left_on="code site",
    right_on="Code",
    how="inner"  # Inner pour garder SEULEMENT les stations match√©es
)

if df_merged.empty:
    print("‚ùå Aucune correspondance entre mesures et stations")
    print(f"Codes dans CSV : {df_measures['code site'].unique()}")
    print(f"Codes dans stations.csv : {df_stations['Code'].unique()}")
    exit(1)

# Filtrer lignes sans coordonn√©es
df_merged = df_merged[df_merged['Latitude'].notna() & df_merged['Longitude'].notna()]
print(f"‚úÖ Merge r√©ussi : {len(df_merged)} lignes avec coordonn√©es")

# Cr√©ation GeoJSON - AGR√âGATION JOURNALI√àRE PAR POLLUANT
print("\nüó∫Ô∏è  Cr√©ation du GeoJSON...")

stations_grouped = df_merged.groupby(['code site', 'nom site', 'Latitude', 'Longitude'])

features = []

for (code_station, nom_station, lat, lon), group in stations_grouped:
    # Grouper par polluant et calculer moyenne + max
    polluants_stats = []
    
    for polluant, polluant_data in group.groupby('Polluant'):
        valeurs = polluant_data['valeur'].dropna()
        
        if len(valeurs) > 0:
            polluants_stats.append({
                "polluant": str(polluant),
                "valeur_moyenne": round(float(valeurs.mean()), 2),
                "valeur_max": round(float(valeurs.max()), 2),
                "valeur_min": round(float(valeurs.min()), 2),
                "nb_mesures": int(len(valeurs)),
                "unite": str(polluant_data['unit√© de mesure'].iloc[0]) if pd.notna(polluant_data['unit√© de mesure'].iloc[0]) else "¬µg/m3",
                "date": str(polluant_data['Date de d√©but'].iloc[0])[:10] if pd.notna(polluant_data['Date de d√©but'].iloc[0]) else "N/A"
            })
    
    # Trier par polluant (alphab√©tique)
    polluants_stats = sorted(polluants_stats, key=lambda x: x['polluant'])
    
    features.append({
        "type": "Feature",
        "geometry": {
            "type": "Point",
            "coordinates": [float(lon), float(lat)]
        },
        "properties": {
            "code_station": str(code_station),
            "nom_station": str(nom_station),
            "nb_polluants": len(polluants_stats),
            "polluants": polluants_stats
        }
    })

geojson = {"type": "FeatureCollection", "features": features}

with open(OUTPUT_GEOJSON, "w", encoding="utf-8") as f:
    json.dump(geojson, f, ensure_ascii=False, indent=2)

total_polluants = sum(len(f['properties']['polluants']) for f in features)
print(f"‚úÖ GeoJSON g√©n√©r√© : {OUTPUT_GEOJSON}")
print(f"   {len(features)} stations, {total_polluants} polluants avec stats journali√®res")














